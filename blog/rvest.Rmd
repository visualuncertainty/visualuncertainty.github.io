---
author: "Abigail Lebrecht"
date: 2018-07-05
title: "Mining Mumsnet Opinions with R and RVest - Part 1"
tags: ["open data","R"]
meta_img: "/img/twitterpic2.jpg"
---
```{r echo=FALSE,warning=FALSE,include=FALSE}
library(tidyverse)
library(rvest)
```

[Mumsnet](http://mumsnet.com) is an incredible source of information and opinions. Providing anonymised and moderated message boards, it is an interesting segment of a large demographic of public opinion. Obviously it struggles from the inherent biases of all social media platforms, but it attracts a different crowd and a less polarising tone than unanonymised platforms such as Facebook and Twitter, helped by careful moderation from MNHQ. Data Scientists commonly mine Facebook and Twitter to measure public opinion, and APIs are available for doing this, but I think Mumsnet conversations can add to the depth of online opinion mining.

There is no Mumsnet API, but conversations can be scraped using the R [Rvest](https://cran.r-project.org/web/packages/rvest/rvest.pdf) package from the [tidyverse](https://www.tidyverse.org/). I want to investigate Mumsnet's opinions on one of the most controversial and polarising topics in the UK currently - Brexit. Fortunately Mumsnet has a special [Brexit talk board](https://www.mumsnet.com/Talk/eu_referendum_2016_), however not all users are aware of this and Brexit related topics are often brought up on other messageboards, primarily [Am I being unreasonable](https://www.mumsnet.com/Talk/am_i_being_unreasonable) and [politics](https://www.mumsnet.com/Talk/politics). In this blogpost I am going to summarise some of the issues that arise when scraping these posts using the Rvest package. 

#Scraping all the relevant pages

We want to collect all posts from every (or as many as possible) conversations about Brexit. We know that all conversations in the **eu_referendums** channel are about Brexit and some in the other channels.

```{r echo=TRUE}
eurefs = 'https://www.mumsnet.com/Talk/eu_referendum_2016_'
aiburefs = 'https://www.mumsnet.com/Talk/am_i_being_unreasonable'
polrefs = 'https://www.mumsnet.com/Talk/politics'
```

First we need to scroll through all the listings in these directories. Before we can do this we need to find out how many pages of listings each directory has. We read in our first listings page and look for the node with the page number listed :

```{r echo=TRUE}

eur = read_html(eurefs, encoding = "UTF-8") 

pages = html_nodes(eur,'.first')%>%
  html_text() %>% unique() %>%str_extract_all( "\\-*\\d+\\.*\\d*") %>% unlist()
```

The node titled **first** contains this and running **html_nodes(eur,'.first')%>%
  html_text() %>% unique()** gives `r html_nodes(eur,'.first')%>%
  html_text() %>% unique()`. The **unique** function is important as often this is repeated at both the top and bottom of the page and we only need it once. We then need to pull out just the digits which we can do using regular expressions. We are then left with a vector of two numbers (masquerading as characters), the first page number and the last page number. Setting these as numerics **i** and **n** we can then generate a character vector of the URIs for all pages in that chatroom.

```{r echo=TRUE}

i=as.numeric(pages[1])
n= as.numeric(pages[2])

uris = paste(eurefs,"?order=&pg=",as.character(seq(i:n)),sep='')

```

We now need to traverse through these uris, and collect the page titles from each one. Thus for each page we need to do the following:

```{r echo=TRUE,eval=FALSE}

 read_html(uri, encoding = "UTF-8") %>% html_nodes('.title')%>%
  html_nodes(xpath = "./a") %>% 
  html_attr("href")

```

This reads all the nodes labelled *title* and pulls out the *href* in the link. Thus giving us a vector of uris of all conversations in the channel. This is fine for the *Brexit* channel, but in *AIBU* and *Politics*, Brexit is not the only topic. So we only want to gather the topics which relate to Brexit. For this I took a rather crude approach of looking for pages with *Brexit* or *EU* in the title:

```{r echo=TRUE,eval=FALSE}

  read_html(uri, encoding = "UTF-8") %>% html_nodes('.title')%>%
    html_nodes(xpath = "//tr[.//a[contains(@href, 'Brexit') 
               or contains(@href, 'brexit') or contains(@href, 'BREXIT') 
               or contains(@href, '-EU-')]]//a") %>% 
    html_attr("href")

```

We can do this using **xpath** to filter the XML nodes before extracting the **href** links. I look for documents with a title containing **Brexit** (with various capitalisation options), or **EU**. This is a slightly crude approach and will include some irrelevant conversations (eg Mumsnet favourite "will Brexit impact house prices?"), and miss many more conversations that may reference the Brexit situation without referring to it. 

Identifying a Brexit themed post could probably be done more accurately using machine learning. We could use the conversation titles from the Brexit talk board to build a model that predicts if a conversation title is likely to be about Brexit (eg Naive Bayes). However, given the small number of conversations in the other channels that this would apply to, it doesn't seem worth the effort.

Another issue with scraping this data is dates. While the Brexit chat board stretches back to only when Brexit became a talking point (March 2016), the Politics and AIBU boards go back much further in time. With my approach we pull every relevant conversation even if it is a from a few years earlier. I then filter by date after reading the contents of conversations. An alternative would be to read the last post date for each conversation and stop reading the uri when a conversation's last posting date is before March 2016. Both these methods are quite slow, although I haven't compared speeds yet.

In the next blogpost we will look at using Rvest to scrape the conversations. I will highlight some of the stylistic issues unique to Mumsnet that need to be considered when reading their data for analysis.